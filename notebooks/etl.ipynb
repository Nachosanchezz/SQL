{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL son las siglas de **Extract, Transform, Load**, es decir, **Extraer**, **Transformar** y **Cargar**. Es un proceso esencial para preparar datos desde diversas fuentes para su análisis posterior. El objetivo principal es:\n",
    "\n",
    "- **Extraer** los datos desde una o más fuentes (por ejemplo, bases de datos locales, en la nube, APIs).\n",
    "- **Transformar** los datos aplicando reglas de negocio, limpieza, agregación o restructuración.\n",
    "- **Cargar** los datos limpios y estructurados en un sistema de destino, como una base de datos analítica o un modelo de datos para reporting o machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de las librerías necesarias para la ejecución del proceso ETL. `pyodbc` se utiliza para la conexión con bases de datos SQL, `pandas` y `numpy` para la manipulación de datos, `os` para operaciones del sistema de archivos, y `warnings` para suprimir mensajes de advertencia innecesarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de los parámetros de conexión para establecer comunicación con una base de datos en la nube de Azure SQL Database. Se especifica el servidor, la base de datos y el driver necesario para autenticar con Azure Active Directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Conexión a **Azure SQL**\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de los parámetros de conexión para una base de datos local utilizando SQL Server. Esta conexión permitirá ejecutar consultas y cargar tablas desde una base de datos on-premise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de conexión\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Conexión a SQL Server local\n",
    "local_conn_local = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se establece la ruta donde se encuentran los scripts SQL para crear o cargar las distintas tablas del modelo dimensional. Además, se crea un diccionario que asocia cada tabla con su archivo SQL correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_folder = \"../database/dimensional\"\n",
    "# Lista de archivos SQL y nombre de tabla destino\n",
    "tablas = {\n",
    "    \"dim_clientes\": \"dim_clientes.sql\",\n",
    "    \"dim_tiempo\": \"dim_tiempo.sql\",\n",
    "    \"dim_producto\": \"dim_producto.sql\",\n",
    "    \"dim_zona\": \"dim_zona.sql\",\n",
    "    \"tabla_hechos\": \"tabla_hechos.sql\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define un diccionario que mapea cada tabla con su clave primaria. Esto será útil más adelante para establecer restricciones de integridad y relaciones entre tablas, especialmente si se cargan en un modelo relacional o se realiza deduplicación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary keys for each table.\n",
    "primary_keys = {\n",
    "    \"tabla_hechos\": [\"CODE\"],\n",
    "    \"dim_clientes\": [\"Customer_ID\"],\n",
    "    \"dim_zona\": [\"TIENDA_ID\"],\n",
    "    \"dim_producto\": [\"Id_Producto\"],\n",
    "    \"dim_tiempo\": [\"Fecha\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta celda se define un diccionario llamado `foreign_keys` que especifica las relaciones entre la tabla principal de hechos (`tabla_hechos`) y las tablas dimensionales del modelo estrella (`dim_clientes`, `dim_zona`, `dim_producto`, `dim_tiempo`). \n",
    "\n",
    "Cada entrada del diccionario asocia una columna de la tabla de hechos con su tabla de referencia, indicando así cómo se deben establecer las claves foráneas en la base de datos. Esto permite mantener la integridad referencial durante la creación de las tablas y garantiza que las relaciones entre entidades estén correctamente representadas dentro del modelo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign keys for each table.\n",
    "foreign_keys = {\n",
    "    \"tabla_hechos\": {\n",
    "        \"Customer_ID\": \"dim_clientes(Customer_ID)\",\n",
    "        \"TIENDA_ID\": \"dim_zona(TIENDA_ID)\",\n",
    "        \"Id_Producto\": \"dim_producto(Id_Producto)\",\n",
    "        \"Sales_Date\": \"dim_tiempo(Fecha)\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función llamada `create_table_sql` que genera automáticamente el script SQL para crear una tabla en la base de datos a partir de un DataFrame de pandas. Esta función detecta el tipo de datos de cada columna (fecha, número entero, flotante o texto) y construye el `CREATE TABLE` con los tipos correspondientes. Además, añade claves primarias y foráneas si se encuentran definidas en los diccionarios `primary_keys` y `foreign_keys`. Esto automatiza la creación de tablas durante el proceso ETL, garantizando que la estructura sea coherente con los datos de origen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_sql(table_name, df):\n",
    "    # Definición de los tipos de datos SQL para cada columna del DataFrame: Por defecto tipo TEXTO.\n",
    "    col_defs = []\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        elif df[col].dtype == np.float32:\n",
    "            col_defs.append(f'[{col}] FLOAT')\n",
    "        elif df[col].dtype == np.int32:\n",
    "            col_defs.append(f'[{col}] INT')\n",
    "        else:\n",
    "            col_defs.append(f'[{col}] NVARCHAR(255)')\n",
    "\n",
    "    # Agregación clave primaria si corresponde.\n",
    "    pk = \", PRIMARY KEY (\" + \", \".join(primary_keys[table_name]) + \")\" if table_name in primary_keys else \"\"\n",
    "    # Agregación claves foráneas si corresponde.\n",
    "    fk = \"\"\n",
    "    if table_name in foreign_keys:\n",
    "        for col, ref in foreign_keys[table_name].items():\n",
    "            fk += f\", FOREIGN KEY ({col}) REFERENCES {ref}\"\n",
    "\n",
    "    return f\"CREATE TABLE {table_name} ({', '.join(col_defs)}{pk}{fk});\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se define una función `drop_tables_in_order` que se encarga de eliminar tablas de la base de datos local en un orden específico. Esto es importante cuando existen relaciones entre tablas (por ejemplo, claves foráneas) que impiden borrarlas en cualquier orden. La función comprueba si la tabla existe antes de intentar eliminarla para evitar errores. Se ejecuta dentro de un bloque `try-except` para manejar posibles errores durante la eliminación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables_in_order(cursor, conn):\n",
    "    drop_order = [\"tabla_hechos\", \"dim_tiempo\", \"dim_producto\", \"dim_zona\", \"dim_clientes\"]\n",
    "    for table in drop_order:\n",
    "        # Verifica si la tabla existe en el esquema actual.\n",
    "        check_exists_query = f\"\"\"\n",
    "        IF OBJECT_ID('{table}', 'U') IS NOT NULL\n",
    "            DROP TABLE {table};\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor.execute(check_exists_query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al intentar eliminar la tabla {table}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta celda representa el núcleo de la ETL (Extracción, Transformación y Carga):\n",
    "\n",
    "- Se establecen conexiones a la base de datos de Azure y a la base de datos local.\n",
    "- Se eliminan las tablas locales existentes usando la función definida anteriormente.\n",
    "- Para cada tabla definida en el diccionario `tablas`, se:\n",
    "  - Ejecuta una consulta SQL contra la base de datos de Azure.\n",
    "  - Carga el resultado en un DataFrame.\n",
    "  - Elimina columnas duplicadas, detecta columnas de fecha y convierte tipos de datos.\n",
    "  - Limpia valores nulos, usando lógicas distintas según el tipo de dato.\n",
    "  - Convierte `float64` y `int64` a `float32` e `int32` para optimización.\n",
    "  - Crea dinámicamente la tabla SQL usando `create_table_sql`.\n",
    "  - Inserta los datos en la base de datos local utilizando `executemany` para eficiencia.\n",
    "\n",
    "Al final, se cierran las conexiones y se indica que el proceso ETL ha finalizado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexiones correctamente establecidas.\n",
      "\n",
      "Procesando: dim_clientes\n",
      "   - Filas obtenidas: 44053\n",
      "   - Columnas: ['Customer_ID', 'CODIGO_POSTAL', 'Edad', 'GENERO', 'RENTA_MEDIA_ESTIMADA', 'STATUS_SOCIAL', 'Fecha_nacimiento', 'ENCUESTA_CLIENTE_ZONA_TALLER', 'ENCUESTA_ZONA_CLIENTE_VENTA', 'poblacion', 'provincia', 'lat', 'lon', 'CP', 'Max_Mosaic', 'Max_Mosaic_G', 'Renta_Media', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
      "   - Tabla dim_clientes creada correctamente.\n",
      "   - 44053 filas insertadas.\n",
      "\n",
      "Procesando: dim_tiempo\n",
      "   - Filas obtenidas: 3652\n",
      "   - Columnas: ['Fecha', 'Año', 'Añomes', 'Mes', 'Dia', 'Diadelasemana', 'Diadelesemana_desc', 'Festivo', 'Findesemana', 'FinMes', 'InicioMes', 'Laboral', 'Mes_desc', 'Semana']\n",
      "   - Tabla dim_tiempo creada correctamente.\n",
      "   - 3652 filas insertadas.\n",
      "\n",
      "Procesando: dim_producto\n",
      "   - Filas obtenidas: 404\n",
      "   - Columnas: ['Id_Producto', 'Code_', 'Modelo', 'Kw', 'TIPO_CARROCERIA', 'TRANSMISION_ID', 'FUEL', 'CATEGORIA_ID', 'Equipamiento', 'Margen', 'Costetransporte', 'Margendistribuidor', 'GastosMarketing']\n",
      "   - Tabla dim_producto creada correctamente.\n",
      "   - 404 filas insertadas.\n",
      "\n",
      "Procesando: dim_zona\n",
      "   - Filas obtenidas: 12\n",
      "   - Columnas: ['ZONA_ID', 'ZONA', 'PROVINCIA_ID', 'PROV_DESC', 'TIENDA_ID', 'TIENDA_DESC']\n",
      "   - Tabla dim_zona creada correctamente.\n",
      "   - 12 filas insertadas.\n",
      "\n",
      "Procesando: tabla_hechos\n",
      "   - Filas obtenidas: 58049\n",
      "   - Columnas: ['CODE', 'Code_', 'Customer_ID', 'PVP', 'COSTE_VENTA_NO_IMPUESTOS', 'IMPUESTOS', 'MANTENIMIENTO_GRATUITO', 'EN_GARANTIA', 'EXTENSION_GARANTIA', 'FIN_GARANTIA', 'Fecha_Fin_Garantia', 'SEGURO_BATERIA_LARGO_PLAZO', 'Id_Producto', 'Sales_Date', 'TIENDA_ID', 'FORMA_PAGO_ID', 'MOTIVO_VENTA_ID', 'ZONA_ID', 'PROVINCIA_ID', 'DATE_ULTIMA_REVISION', 'DIAS_DESDE_ULTIMA_REVISION', 'Km_medio_por_revision', 'km_ultima_revision', 'Revisiones', 'Costetransporte', 'GastosMarketing', 'Margen', 'Margendistribuidor', 'Comisión_marca', 'Fue_Lead', 'Lead_compra', 'Logistic_date', 'Prod_date', 't_logist_days', 't_prod_date', 't_stock_dates', 'Origen', 'Car_Age', 'DIAS_DESDE_LA_ULTIMA_ENTRADA_TALLER', 'DIAS_EN_TALLER', 'QUEJA', 'Margen_eur_bruto', 'Margen_eur', 'Tasa_Churn']\n",
      "   - Tabla tabla_hechos creada correctamente.\n",
      "   - 58049 filas insertadas.\n",
      "\n",
      "ETL completado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Conexión a las bases de datos.\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    conn_local = pyodbc.connect(local_conn_local)\n",
    "    print(\"Conexiones correctamente establecidas.\\n\")\n",
    "\n",
    "    with conn_local.cursor() as cursor:\n",
    "        drop_tables_in_order(cursor, conn_local)\n",
    "    # Procesamiento de cada tabla definida en el diccionario de Queries.\n",
    "    for table_name, file in tablas.items():\n",
    "        print(f\"Procesando: {table_name}\")\n",
    "        query_path = os.path.join(query_folder, file)\n",
    "        with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_query = f.read()\n",
    "\n",
    "        # Ejecución de la consulta sobre la base de datos de Azure.\n",
    "        df = pd.read_sql(sql_query, conn_azure)\n",
    "\n",
    "        # Eliminación de las columnas duplicadas.\n",
    "        if df.columns.duplicated().any():\n",
    "            print(f\"Columnas duplicadas en {table_name}: {df.columns[df.columns.duplicated()].tolist()}\")\n",
    "            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "        # Detección de las columnas tipo DATE para convertirlas adecuadamente.\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object or df[col].dtype == \"string\":\n",
    "                sample_values = df[col].astype(str).sample(min(len(df), 30), random_state=42)\n",
    "                # Saltar si parece una columna numérica (para no confundir INT con DATE).\n",
    "                if sample_values.str.isdigit().mean() > 0.8:\n",
    "                    continue\n",
    "                try:\n",
    "                    parsed = pd.to_datetime(sample_values, errors='coerce')\n",
    "                    if parsed.notna().sum() > 0.9 * len(sample_values):\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                except:\n",
    "                    pass\n",
    "        # Si el DataFrame está vacío, se salta.\n",
    "        if df.empty:\n",
    "            print(f\"La tabla {table_name} no devolvió resultados.\\n\")\n",
    "            continue\n",
    "        print(f\"   - Filas obtenidas: {df.shape[0]}\")\n",
    "        print(f\"   - Columnas: {df.columns.tolist()}\")\n",
    "\n",
    "        # Limpieza de valores nulos y tipos de datos.\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True) # Reemplazar espacios en blanco por NaN.\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                # Valor sentinel (ej: -1 o 999999).\n",
    "                sentinel = -1\n",
    "                df[col] = df[col].fillna(sentinel)\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].mode(dropna=True)[0])\n",
    "            else:\n",
    "                df[col] = df[col].fillna(\"N/A\")\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "        # Creación de la tabla en la base de datos local.\n",
    "        with conn_local.cursor() as cursor:\n",
    "            create_sql = create_table_sql(table_name, df)\n",
    "            cursor.execute(create_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla {table_name} creada correctamente.\")\n",
    "\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "            print(f\"   - {df.shape[0]} filas insertadas.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'conn_azure' in locals():\n",
    "        conn_azure.close()\n",
    "    if 'conn_local' in locals():\n",
    "        conn_local.close()\n",
    "\n",
    "print(\"ETL completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
